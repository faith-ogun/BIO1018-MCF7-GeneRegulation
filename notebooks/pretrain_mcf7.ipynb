{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # (Continue) Pretraining a GET Model on MCF-7 ATAC\n",
    "\n",
    "\n",
    " This tutorial demonstrates how to train a GET model to predict ATAC-seq peaks using motif information. We'll cover:\n",
    "\n",
    " 1. Loading and configuring the model\n",
    "\n",
    " 2. Training without a pretrained checkpoint\n",
    "\n",
    " 3. Training with a pretrained checkpoint\n",
    "\n",
    " 4. Comparing the results\n",
    "\n",
    "\n",
    "\n",
    " ## Setup\n",
    "\n",
    " First, let's import the necessary modules and set up our configuration.\n",
    " \n",
    " Note:\n",
    " If you run from a Mac, make sure you use the jupyter notebook rather than the VSCode interactive python editor as the later seems to have issue with multiple workers.\n",
    " If you run from Linux, both should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from get_model.config.config import load_config, pretty_print_config\n",
    "from get_model.run_region import run_zarr as run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Configuration\n",
    "\n",
    "\n",
    "\n",
    " We'll start by loading a predefined configuration and customizing it for our needs.\n",
    "\n",
    " The base configuration is in `get_model/config/finetune_tutorial.yaml`\n",
    "\n",
    " This has been altered to allow for multiple zarr directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on all Chromsomes\n",
    "First pass with all chromosomes. \n",
    "The trainer cannot register individual chromsomes or ATAC_only. It requres both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load the configuration for fine-tuning\n",
    "cfg = load_config('finetune_tutorial')\n",
    "\n",
    "# Set dataset parameters\n",
    "cfg.dataset.zarr_path = \"/project/home/p200469/get_BIO1018/get_preprocess_output.zarr/\"\n",
    "cfg.dataset.celltypes = \"all_chrs\"\n",
    "cfg.dataset.leave_out_chromosomes = None  # Include all chromosomes in training\n",
    "\n",
    "# Set project parameters\n",
    "cfg.run.project_name = 'pretrain_all_chrs'\n",
    "cfg.run.use_wandb = True  # Enable logging with Weights & Biases\n",
    "\n",
    "# Training configuration\n",
    "cfg.training.epochs = 50  # Number of training epochs\n",
    "cfg.training.val_check_interval = 1.0  # Validate after each epoch\n",
    "\n",
    "# Debugging information\n",
    "print(\"Configuration Loaded Successfully!\")\n",
    "print(f\"Zarr path: {cfg.dataset.zarr_path}\")\n",
    "print(f\"Project Name: {cfg.run.project_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model Selection\n",
    "\n",
    "\n",
    " We'll use the GETRegionPretrain model, which is designed to use contextual motif(+atac) information to target motif(+atac) information\n",
    "\n",
    " This model is particularly useful for understanding the relationship between motifs and chromatin accessibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Switch model to finetune ATAC model\n",
    "cfg.model = load_config('model/GETRegionPretrain').model.model\n",
    "cfg.dataset.mask_ratio = 0.5 # mask 50% of the motifs. This has to be set for pretrain dataloader to generate proper mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Without Pretraining Checkpoint\n",
    "\n",
    "\n",
    "\n",
    " First, let's train the model from scratch (without using a pretrained checkpoint).\n",
    "\n",
    " This will give us a baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Tell machine output directory\n",
    "cfg.machine.output_dir = \"/project/home/p200469/get_BIO1018/get_ML_output\"\n",
    "\n",
    "# first run the model without initializing with a pretrain checkpoint\n",
    "cfg.run.run_name='pretrain_MCF7_scratch' # this is a unique name for this run\n",
    "cfg.finetune.checkpoint = None\n",
    "cfg.finetune.use_lora = False\n",
    "cfg.run.use_wandb = True\n",
    "trainer = run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Continue Training With Pretrained Checkpoint Using LoRA\n",
    "\n",
    "\n",
    "\n",
    " Now, let's train the model using a pretrained checkpoint. This checkpoint was trained on a large dataset\n",
    "\n",
    " and should help the model learn faster and potentially achieve better performance.\n",
    "\n",
    "\n",
    "\n",
    " Note: You'll need to download the checkpoint first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Download pulbic checkpoint file\n",
    "\n",
    "!curl -O https://2023-get-xf2217.s3.amazonaws.com/get_demo/checkpoints/regulatory_inference_checkpoint_fetal_adult/pretrain_fetal_adult/checkpoint-799.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# now train the model with a pretrain checkpoint\n",
    "\n",
    "cfg.machine.output_dir = \"/project/home/p200469/get_BIO1018/get_ML_output\"\n",
    "cfg.finetune.checkpoint = './checkpoint-799.pth'\n",
    "cfg.run.run_name = 'pretrain_mcf7_from_pretrain_lora'\n",
    "cfg.finetune.model_key = \"model\"\n",
    "cfg.finetune.rename_config = {\n",
    "  \"encoder.head.\": \"head_mask.\",\n",
    "  \"encoder.region_embed\": \"region_embed\",\n",
    "  \"region_embed.proj.\": \"region_embed.embed.\",\n",
    "  \"encoder.cls_token\": \"cls_token\",\n",
    "}\n",
    "cfg.finetune.strict = True\n",
    "cfg.finetune.use_lora = True\n",
    "cfg.finetune.layers_with_lora = ['region_embed', 'encoder']\n",
    "trainer = run(cfg)\n",
    "trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# now train the model with a pretrain checkpoint without using LoRA\n",
    "cfg.finetune.checkpoint = './checkpoint-799.pth'\n",
    "cfg.run.run_name = 'pretrain_mcf7_from_pretrain_no_lora'\n",
    "cfg.finetune.use_lora = False\n",
    "trainer = run(cfg)\n",
    "trainer.callback_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
